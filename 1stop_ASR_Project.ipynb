{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1stop ASR Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOqprVOcH2CLXwHs0ngKqu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SachiSachu/1stop/blob/main/1stop_ASR_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Visualizing Audio Signals - Reading from a File and Working on it**"
      ],
      "metadata": {
        "id": "IstOJhIQKGpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-> Now, read the stored audio file. It will return two values: the sampling frequency and the audio signal. Provide the path of the audio file where it is stored.\n",
        "\n",
        "2-> Display the parameters like sampling frequency of the audio signal, data type of signal and its duration, using the commands\n",
        "\n",
        "3-> This step involves normalizing the signal\n",
        "\n",
        "4-> In this step, we are extracting the first 100 values from this signal to visualize.\n",
        "\n",
        "5-> Now, visualize the signal using the commands."
      ],
      "metadata": {
        "id": "zP9cUHOhKace"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile \n",
        "\n",
        "#1\n",
        "frequency, audio = wavfile.read(\"/Users/Sachu/audio.wav\")\n",
        "#2\n",
        "print('nSignal shape:', audio.shape)\n",
        "print('Signal Datatype:', audio.dtype)\n",
        "print('Signal duration:', round(audio.shape[0] /\n",
        "float(frequency), 2), 'seconds')\n",
        "#3\n",
        "audio = audio / np.power(2, 15)\n",
        "#4\n",
        "audio = audio [:100]\n",
        "time_label = 1000 * np.arange(0, len(signal), 1) / float(frequency)\n",
        "#5\n",
        "plt.plot(time, signal, color='blue')\n",
        "plt.xlabel('Time (milliseconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Input audio signal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7y1V_hgsCpy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Characterizing the Audio Signal: Transforming to Frequency Domain**"
      ],
      "metadata": {
        "id": "PFyh-QluLpYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1->Now, read the stored audio file. It will return two values: the sampling frequency and the the audio signal. Provide the path of the audio file where it is stored.\n",
        "\n",
        "2->In this step, we will display the parameters like sampling frequency of the audio signal, data type of signal and its duration, using the commands\n",
        "\n",
        "3->we need to normalize the signal\n",
        "\n",
        "4->This step involves extracting the length and half length of the signal.\n",
        "\n",
        "5->Now, we need to apply mathematics tools for transforming into frequency domain. Here we are using the Fourier Transform\n",
        "\n",
        "6->do the normalization of frequency domain signal and square it −\n",
        "\n",
        "7->Next, extract the length and half length of the frequency transformed signal \n",
        "\n",
        "8->Note that the Fourier transformed signal must be adjusted for even as well as odd case\n",
        "\n",
        "9->Now, extract the power in decibal(dB)\n",
        "\n",
        "10->Adjust the frequency in kHz for X-axis −\n",
        "\n",
        "11->Now, visualize the characterization of signal"
      ],
      "metadata": {
        "id": "WgWBFeGSL5JO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "frequency, audio = wavfile.read(\"/Users/Sachu/audio.wav\")\n",
        "#2\n",
        "print('nSignal shape:', audio.shape)\n",
        "print('Signal Datatype:', audio.dtype)\n",
        "print('Signal duration:', round(audio.shape[0] /\n",
        "float(frequency), 2), 'seconds')\n",
        "#3\n",
        "audio = audio / np.power(2, 15)\n",
        "#4\n",
        "signal_length = len(audio_signal)\n",
        "half_length = np.ceil((signal_length + 1) / 2.0).astype(np.int)\n",
        "#5\n",
        "SignalFrequency = np.fft.fft(audio)\n",
        "#6\n",
        "SignalFrequency = abs(SignalFrequency[0:half_length]) / signal_length\n",
        "SignalFrequency **= 2\n",
        "#7\n",
        "len_fts = len(SignalFrequency)\n",
        "#8\n",
        "if signal_length % 2:\n",
        "  SignalFrequency[1:len_fts] *= 2\n",
        "else:\n",
        "  SignalFrequency[1:len_fts-1] *= 2\n",
        "#9\n",
        "signal_power = 10 * np.log10(SignalFrequency)\n",
        "#10\n",
        "x_axis = np.arange(0, len_half, 1) * (SignalFrequency / signal_length) / 1000.0\n",
        "#11\n",
        "plt.figure()\n",
        "plt.plot(x_axis, signal_power, color='black')\n",
        "plt.xlabel('Frequency (kHz)')\n",
        "plt.ylabel('Signal power (dB)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "It6_FB9vLvR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Generating Monotone Audio Signal**"
      ],
      "metadata": {
        "id": "jwFVAt-QIQpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1->Provide the file where the output file should be saved\n",
        "\n",
        "2->Now, specify the parameters of your choice\n",
        "\n",
        "3-> we can generate the audio signal\n",
        "\n",
        "4->save the audio file in the output file\n",
        "\n",
        "5->Extract the first 100 values for our graph\n",
        "\n",
        "6->Now, visualize the generated audio signal "
      ],
      "metadata": {
        "id": "J-jKR3nrT38q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "#1\n",
        "output= 'audio_signal_generated.wav'\n",
        "\n",
        "#2\n",
        "duration = 4 # in seconds\n",
        "frequency = 44100 # in Hz\n",
        "frequency_tone = 784\n",
        "min = -4 * np.pi\n",
        "max = 4 * np.pi\n",
        "\n",
        "#3\n",
        "t = np.linspace(min, max, duration * frequency)\n",
        "audio = np.sin(2 * np.pi * tone_freq * t)\n",
        "#4\n",
        "write(output, frequency, signal_scaled)\n",
        "\n",
        "#5\n",
        "audio= audio[:100]\n",
        "time_axis = 1000 * np.arange(0, len(signal), 1) / float(sampling_freq)\n",
        "\n",
        "#6\n",
        "plt.plot(time_axis, signal, color='blue')\n",
        "plt.xlabel('Time in milliseconds')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Generated audio signal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94CcZZDxINSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction from Speech**"
      ],
      "metadata": {
        "id": "1Hpz_wlAIsgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1->Now, read the stored audio file. It will return two values − the sampling frequency and the audio signal. Provide the path of the audio file where it is stored.\n",
        "\n",
        "2->Note that here we are taking first 15000 samples for analysis\n",
        "\n",
        "3->Use the MFCC techniques and execute the following command to extract the MFCC features −\n",
        "\n",
        "4->Now, print the MFCC parameters, as shown −\n",
        "\n",
        "5->Now, plot and visualize the MFCC features using the commands given below −\n",
        "\n",
        "6->In this step, we work with the filter bank features as shown −\n",
        "Extract the filter bank features \n",
        "\n",
        "7->Now, print the filterbank parameters.\n",
        "\n",
        "8->Now, plot and visualize the filterbank features"
      ],
      "metadata": {
        "id": "SdDP7BQkU1Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "from python_speech_features import mfcc, logfbank\n",
        "\n",
        "#1\n",
        "frequency, audio = wavfile.read(\"/windows/Sachu/audio.wav\")\n",
        "\n",
        "#2\n",
        "audio = audio[:15000]\n",
        "\n",
        "#3\n",
        "featuresMfcc = mfcc(audio_signal, frequency)\n",
        "\n",
        "#4\n",
        "print('nMFCC:nNumber of windows =', featuresMfcc.shape[0])\n",
        "print('Length of each feature =', featuresMfcc.shape[1])\n",
        "\n",
        "#5\n",
        "featuresMfcc = featuresMfcc.T\n",
        "plt.matshow(featuresMfcc)\n",
        "plt.title('MFCC')\n",
        "\n",
        "#6\n",
        "filterbankFeatures = logfbank(audio, frequency)\n",
        "\n",
        "#7\n",
        "print('nFilter bank:nNumber of windows =', filterbankFeatures.shape[0])\n",
        "print('Length of each feature =', filterbankFeatures.shape[1])\n",
        "\n",
        "#8\n",
        "filterbankFeatures = filterbankFeatures.T\n",
        "plt.matshow(filterbankFeatures)\n",
        "plt.title('Filter bank')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXfgIo9TIrsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recognition of Spoken Words**"
      ],
      "metadata": {
        "id": "fi375dDTJiNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1->Create an object\n",
        "\n",
        "2->Now, the Microphone() module will take the voice as input\n",
        "\n",
        "3->Now google API would recognize the voice and gives the output.\n",
        "\n",
        "4->You can see the following output −\n",
        "\n",
        "Please Say Something:\n",
        "\n",
        "You said:"
      ],
      "metadata": {
        "id": "pdaQ3AAjW_XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "#1\n",
        "recording = sr.Recognizer()\n",
        "\n",
        "#2\n",
        "with sr.Microphone() as source: recording.adjust_for_ambient_noise(source)\n",
        "print(\"Please Say something:\")\n",
        "audio = recording.listen(source)\n",
        "\n",
        "#3\n",
        "try:\n",
        "  print(\"You said: n\" + recording.recognize_google(audio))\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "L2TrF8TiJkD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}